{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d950b235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, concatenate\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5da496f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'pd_speech_features.csv' downloaded successfully!\n",
      "CSV downloaded and loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/dcleres/Parkinson_Disease_ML/refs/heads/master/pd_speech_features.csv'\n",
    "file_name = 'pd_speech_features.csv'\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    with open(file_name, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"File '{file_name}' downloaded successfully!\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error: Failed to download or find the file '{file_name}'. Exception: {e}\")\n",
    "\n",
    "try:\n",
    "    pd_speech_features = pd.read_csv(file_name)\n",
    "    print(\"CSV downloaded and loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Failed to download or find the file '{file_name}'\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b736094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = pd_speech_features.iloc[0]\n",
    "pd_speech_features = pd_speech_features[1:]\n",
    "pd_speech_features.columns = header\n",
    "pd_speech_features = pd_speech_features.apply(pd.to_numeric)\n",
    "pd_speech_features = pd_speech_features.astype(float)\n",
    "pd_speech_features[['id', 'numPulses', 'numPeriodsPulses']] = pd_speech_features[['id', 'numPulses', 'numPeriodsPulses']].astype(int)\n",
    "pd_speech_features[['gender', 'class']] = pd_speech_features[['gender', 'class']].astype('category')\n",
    "\n",
    "labels = pd_speech_features['class'].astype(int)\n",
    "person_ids = pd_speech_features['id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c193fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU(s) detected: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 769903743258958886\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3676307456\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 13508672362284452469\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3050 6GB Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"✅ GPU(s) detected:\", gpus)\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(\"Memory growth setting failed:\", e)\n",
    "else:\n",
    "    print(\"⚠️ No GPU found. Training will run on CPU.\")\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc467490",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfccs = pd_speech_features.iloc[:, 22:84].astype(float)\n",
    "wavelets = pd_speech_features.iloc[:, 84:148].astype(float)\n",
    "tqwts = pd_speech_features.iloc[:, 148:].astype(float)\n",
    "\n",
    "# Normalize\n",
    "scaler = MinMaxScaler()\n",
    "mfccs_scaled = scaler.fit_transform(mfccs)\n",
    "wavelets_scaled = scaler.fit_transform(wavelets)\n",
    "tqwts_scaled = scaler.fit_transform(tqwts)\n",
    "\n",
    "# ✅ Step 1: Convert to float32 to reduce memory\n",
    "mfccs_scaled = mfccs_scaled.astype(np.float32)\n",
    "wavelets_scaled = wavelets_scaled.astype(np.float32)\n",
    "tqwts_scaled = tqwts_scaled.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1460653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total folds (equal to total unique persons): 252\n"
     ]
    }
   ],
   "source": [
    "logo = LeaveOneGroupOut()\n",
    "splits = list(logo.split(mfccs_scaled, labels, groups=person_ids))\n",
    "print(f\"Total folds (equal to total unique persons): {len(np.unique(person_ids))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7236f32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_level_cnn(input_shapes):\n",
    "    inputs, branches = [], []\n",
    "    for shape in input_shapes:\n",
    "        inp = Input(shape=shape)\n",
    "        x = Conv1D(32, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(inp)\n",
    "        x = Conv1D(32, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "        x = MaxPooling1D(pool_size=2)(x)\n",
    "        x = Conv1D(64, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "        x = Conv1D(64, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "        x = MaxPooling1D(pool_size=2)(x)\n",
    "        x = Conv1D(128, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "        x = Conv1D(128, kernel_size=3, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "        x = MaxPooling1D(pool_size=2)(x)\n",
    "        x = Flatten()(x)\n",
    "        inputs.append(inp)\n",
    "        branches.append(x)\n",
    "\n",
    "    merged = concatenate(branches)\n",
    "    merged = Dropout(0.3)(merged)\n",
    "    merged = Dense(64, activation='relu')(merged)\n",
    "    output = Dense(1, activation='sigmoid', dtype='float32')(merged)  # float32 because of mixed precision\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "total_folds = len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbdf8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Fold 1 / 252: Training...\n"
     ]
    }
   ],
   "source": [
    "for fold_counter, (train_idx, test_idx) in enumerate(splits, start=1):\n",
    "    print(f\"\\n🔄 Fold {fold_counter} / {total_folds}: Training...\")\n",
    "\n",
    "    X_train_mfcc = mfccs_scaled[train_idx]\n",
    "    X_test_mfcc = mfccs_scaled[test_idx]\n",
    "    X_train_wavelet = wavelets_scaled[train_idx]\n",
    "    X_test_wavelet = wavelets_scaled[test_idx]\n",
    "    X_train_tqwt = tqwts_scaled[train_idx]\n",
    "    X_test_tqwt = tqwts_scaled[test_idx]\n",
    "\n",
    "    X_train = [np.expand_dims(X_train_mfcc, axis=2),\n",
    "               np.expand_dims(X_train_wavelet, axis=2),\n",
    "               np.expand_dims(X_train_tqwt, axis=2)]\n",
    "    X_test = [np.expand_dims(X_test_mfcc, axis=2),\n",
    "              np.expand_dims(X_test_wavelet, axis=2),\n",
    "              np.expand_dims(X_test_tqwt, axis=2)]\n",
    "    y_train, y_test = labels.iloc[train_idx], labels.iloc[test_idx]\n",
    "\n",
    "    # ✅ Use tf.data pipeline with smaller batch size\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((tuple(X_train), y_train))\n",
    "    train_dataset = train_dataset.shuffle(1024).batch(16).prefetch(tf.data.AUTOTUNE)  # ✅ Step 2: reduced batch size\n",
    "\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((tuple(X_test), y_test))\n",
    "    test_dataset = test_dataset.batch(16).prefetch(tf.data.AUTOTUNE)  # ✅ Step 2: reduced batch size\n",
    "\n",
    "\n",
    "    model = build_model_level_cnn([x.shape[1:] for x in X_train])\n",
    "    model.fit(train_dataset, epochs=200, verbose=0)\n",
    "\n",
    "    y_pred = (model.predict(test_dataset).flatten() > 0.5).astype(int)\n",
    "    all_preds.extend(y_pred)\n",
    "    all_labels.extend(y_test)\n",
    "\n",
    "    print(f\"✅ Fold {fold_counter}: Accuracy = {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "mcc = matthews_corrcoef(all_labels, all_preds)\n",
    "\n",
    "print(f\"\\n🏁 Full LOPO Results After All Folds:\")\n",
    "print(f\"🔹 Accuracy : {accuracy:.4f}\")\n",
    "print(f\"🔹 F1-Score : {f1:.4f}\")\n",
    "print(f\"🔹 MCC      : {mcc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
